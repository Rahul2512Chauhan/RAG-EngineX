# ğŸ” RAG-EngineX

**RAG-EngineX** is a professional-grade, modular, and fully customizable **Retrieval-Augmented Generation (RAG)** framework for document-based Q&A. It features advanced chunking strategies, vector-based search, reranking with cross-encoders, LangSmith-powered observability, and detailed evaluation using ARES, faithfulness, and relevance metrics â€” all wrapped in an intuitive **Streamlit UI**.

---

## ğŸš€ Demo

ğŸ”— **[Live Demo](https://your-deployed-url.com)** (replace with actual Render/Streamlit link)  
ğŸ¥ **[Watch Walkthrough](https://your-demo-video-link.com)** (replace with video link)

---

## âœ¨ Features

- ğŸ“„ **Multi-format Document Loader** â€“ Load PDFs and other formats with modular loader components.

- ğŸ§© **Custom Chunking Strategies** â€“ Choose from character, recursive, or semantic chunking.

- ğŸ§  **Pluggable Embedding Models** â€“ Support for `sentence-transformers`, `OpenAI`, or custom models.

- ğŸ“¦ **Vector Store Flexibility** â€“ Use FAISS, Chroma, or any LangChain-supported store.

- ğŸ” **Reranking Module** â€“ Improve retrieval relevance with cross-encoder rerankers.

- ğŸ§ª **Evaluation Metrics** â€“ Faithfulness, relevance, ARES, and custom scoring.

- ğŸ“Š **LangSmith Integration** â€“ Trace every prompt-response interaction with latency tracking.

- ğŸ’» **Streamlit UI** â€“ Clean, interactive, and exportable frontend interface.

- ğŸ› ï¸ **Fully Modular Codebase** â€“ Easy to extend and integrate with other pipelines.

- ğŸ“¤ **Answer Export** â€“ Download results in JSON/CSV for audits or further analysis.

---

## ğŸ§  Architecture

RAG-EngineX follows a modular pipeline from loading documents to evaluating responses, with each component decoupled for full flexibility.

### ğŸ” LangSmith Tracing
LangSmith integration enables detailed observability of each RAG step, capturing inputs, outputs, latency, and metadata.

![LangSmith Integration](./assets/LangSmith.png)

---

### ğŸ” Reranker Module
After vector similarity search, a reranker (cross-encoder) reorders retrieved chunks based on semantic relevance.

![Reranker Architecture](./assets/Reranker.png)

---

### ğŸ“Š Evaluation Metrics
Evaluate answer quality using multiple metrics:
- **Faithfulness**: Measures hallucination.
- **Relevance**: Measures contextual alignment.
- **ARES**: Embedding-based answer-scoring technique.

![Evaluation Metrics](./assets/Evaluation%20Metrices.png)

---

### ğŸ’» Streamlit UI
A simple and elegant frontend to enter queries, inspect retrieved contexts, view answers, examine evaluation metrics, and export results.

![Streamlit UI](./assets/UI.png)

---

## âš™ï¸ Tech Stack
![alt text](image.png)
---

## ğŸ“ Project Structure

rag-enginex/
â”‚
â”œâ”€â”€ rag_enginex/ # Core pipeline logic
â”‚ â”œâ”€â”€ loader.py # Document loading
â”‚ â”œâ”€â”€ chunker.py # Chunking logic
â”‚ â”œâ”€â”€ embedder.py # Embedding generation
â”‚ â”œâ”€â”€ vector_store.py # Vector DB interaction
â”‚ â”œâ”€â”€ reranker.py # Cross-encoder reranker
â”‚ â”œâ”€â”€ llm_answer.py # LLM query and generation
â”‚ â”œâ”€â”€ evaluator.py # Automated eval (ARES etc.)
â”‚ â”œâ”€â”€ evaluator_manual.py # Manual scoring 
â”‚ â”œâ”€â”€ llm_wrapper.py # LLM abstraction
â”‚ â””â”€â”€ pipeline.py # Orchestrates entire flow
â”‚
â”œâ”€â”€ ui.py # Streamlit UI
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ runtime.txt # Python version for Render
â”œâ”€â”€ Procfile # Web server entry point
â”œâ”€â”€ .streamlit/
â”‚ â””â”€â”€ config.toml # Theme and server settings
â””â”€â”€ assets/
  â”œâ”€â”€ LangSmith.png
  â”œâ”€â”€ Reranker.png
  â”œâ”€â”€ Evaluation Metrices.png
  â””â”€â”€ UI.png



---

## ğŸ› ï¸ Setup & Deployment

### ğŸ”§ Local Development

```bash

# 1. Clone the repo
git clone https://github.com/yourusername/rag-enginex.git
cd rag-enginex

# 2. Create a virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run locally
streamlit run ui.py

ğŸš€ Deployment on Render.com

Full guide available here: Deploying RAG-EngineX to Render

Basic steps:

Push code to GitHub.

Create new Web Service on Render.

Set build command to :
    pip install -r requirements.txt.

Set start command to :
    streamlit run ui.py --server.port 10000.

Add runtime.txt with Python version.

Add Procfile with:

    web: streamlit run ui.py --server.port $PORT

(Optional) Set up .streamlit/config.toml to customize theme.

ğŸ“¤ Export & Evaluation Features:

    âœ… Faithfulness and relevance scores shown in collapsible sections.

    âœ… Evaluation results (JSON/CSV) downloadable via UI.

    âœ… Easily switch between auto-eval and manual evaluation.

âœï¸ Blogs & Write-ups
ğŸ“˜ Coming Soon on Hashnode

    â€œBuilding a Modular RAG Pipeline from Scratchâ€

    â€œHow to Evaluate RAG Systems Without OpenAIâ€

    â€œStreamlit + LangChain: Perfect UI for Document QAâ€

ğŸ¤ Contributing
    Contributions are welcome! Feel free to submit issues or pull requests.

ğŸ“œ License
    MIT License Â© Rahul Chauhan

ğŸ™Œ Acknowledgments
    LangChain for modular building blocks

    LangSmith for observability

    HuggingFace & Sentence-Transformers for embeddings

    Papers like ARES & RAGAS for evaluation inspiration

---

---

âœ… **Next Steps (optional)**:
- I can save this as `README.md` and export it for your GitHub.
- Help you write the Render deployment guide mentioned above.
- Help you turn this into a blog post on Hashnode.

Would you like me to generate the actual `README.md` file for download now?

