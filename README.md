# ğŸ” RAG-EngineX

**RAG-EngineX** is a professional-grade, modular, and fully customizable **Retrieval-Augmented Generation (RAG)** framework for document-based Q&A. It features advanced chunking strategies, vector-based search, reranking with cross-encoders, LangSmith-powered observability, and detailed evaluation using ARES, faithfulness, and relevance metrics â€” all wrapped in an intuitive **Streamlit UI**.

---

## ğŸš€ Demo

ğŸ”— **Try it now:** [ğŸ‘‰ RAG-EngineX on Render](https://rag-enginex.onrender.com)


ğŸ¥ **Watch Walkthrough Video (2 min)**  


[![Watch the demo](./assets/UI.png)](https://www.loom.com/share/d76911e2d84440f58a7032df410bd317?sid=aef04d04-d16d-4145-9a58-52944d53fc6c)

---

## âœ¨ Features

- ğŸ“„ **Multi-format Document Loader** â€“ Load PDFs and other formats with modular loader components.

- ğŸ§© **Custom Chunking Strategies** â€“ Choose from character, recursive, or semantic chunking.

- ğŸ§  **Pluggable Embedding Models** â€“ Support for `sentence-transformers`, `OpenAI`, or custom models.

- ğŸ“¦ **Vector Store Flexibility** â€“ Use FAISS, Chroma, or any LangChain-supported store.

- ğŸ” **Reranking Module** â€“ Improve retrieval relevance with cross-encoder rerankers.

- ğŸ§ª **Evaluation Metrics** â€“ Faithfulness, relevance, ARES, and custom scoring.

- ğŸ“Š **LangSmith Integration** â€“ Trace every prompt-response interaction with latency tracking.

- ğŸ’» **Streamlit UI** â€“ Clean, interactive, and exportable frontend interface.

- ğŸ› ï¸ **Fully Modular Codebase** â€“ Easy to extend and integrate with other pipelines.

- ğŸ“¤ **Answer Export** â€“ Download results in JSON/CSV for audits or further analysis.

---

## ğŸ§  Architecture

RAG-EngineX follows a modular pipeline from loading documents to evaluating responses, with each component decoupled for full flexibility.

### 1ï¸âƒ£ ğŸ“Œ High-Level Pipeline Overview

Shows the flow of a document through the complete RAG-EngineX pipeline:

![Pipeline Overview](./assets/pipeline_overview.png)


---

### 2ï¸âƒ£ ğŸ¤– Reranker Subsystem

The reranker improves retrieval quality using a BGE cross-encoder before passing results to the LLM:

![Reranker Architecture](./assets/reranker_architecture.png)

---

### 3ï¸âƒ£ ğŸ“Š LangSmith + Evaluation Module

Traces and evaluates generated answers using LangSmith and in-app scoring (ARES / Relevance / Faithfulness):

![LangSmith Evaluation](./assets/langsmith_evaluation.png)

---

## ğŸ–¼ UI & Screenshots

The Streamlit UI is designed for interactivity and insight. It shows answers, sources, evaluation metrics, and supports exporting results.

### ğŸ” LangSmith Trace Viewer

![LangSmith Trace](./assets/LangSmith.png)

---

### ğŸ“ˆ Evaluation Metrics View (ARES / Relevance / Faithfulness)

![Evaluation Metrics](./assets/Evaluation%20Metrices.png)

---

### ğŸ§  Reranker Comparison View

![Reranker Screenshot](./assets/Reranker.png)

---

### ğŸ–¥ï¸ Main Streamlit App UI

![Streamlit UI](./assets/UI.png)


---

## âš™ï¸ Tech Stack
![Tech Stack](./assets/tech_stack.png)
---

```
## ğŸ“ Project Structure

rag-enginex/
â”‚
â”œâ”€â”€ rag_enginex/             # Core pipeline logic
â”‚   â”œâ”€â”€ loader.py            # Document loading
â”‚   â”œâ”€â”€ chunker.py           # Chunking logic
â”‚   â”œâ”€â”€ embedder.py          # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py      # Vector DB interaction
â”‚   â”œâ”€â”€ reranker.py          # Cross-encoder reranker
â”‚   â”œâ”€â”€ llm_answer.py        # LLM query and generation
â”‚   â”œâ”€â”€ evaluator.py         # Automated eval (ARES etc.)
â”‚   â”œâ”€â”€ evaluator_manual.py  # Manual scoring 
â”‚   â”œâ”€â”€ llm_wrapper.py       # LLM abstraction
â”‚   â””â”€â”€ pipeline.py          # Orchestrates entire flow
â”‚
â”œâ”€â”€ ui.py                    # Streamlit UI
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ runtime.txt              # Python version for Render
â”œâ”€â”€ Procfile                 # Web server entry point
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml          # Theme and server settings
â””â”€â”€ assets/
    â”œâ”€â”€ LangSmith.png
    â”œâ”€â”€ Reranker.png
    â”œâ”€â”€ Evaluation Metrices.png
    â”œâ”€â”€ UI.png
    â”œâ”€â”€ pipeline_overview.png
    â”œâ”€â”€ reranker_architecture.png
    â”œâ”€â”€ langsmith_evaluation.png
    â””â”€â”€ tech_stack.png

```


---
```
## ğŸ› ï¸ Setup & Deployment

### ğŸ”§ Local Development



 1. Clone the repo
git clone https://github.com/yourusername/rag-enginex.git
cd rag-enginex

 2. Create a virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

 3. Install dependencies
pip install -r requirements.txt

 4. Run locally
streamlit run ui.py
```

```
ğŸš€ Deployment on Render.com

Full guide available here: Deploying RAG-EngineX to Render

Basic steps:

Push code to GitHub.

Create new Web Service on Render.

Set build command to :
    pip install -r requirements.txt.

Set start command to :
    streamlit run ui.py --server.port 10000.

Add runtime.txt with Python version.

Add Procfile with:

    web: streamlit run ui.py --server.port $PORT

(Optional) Set up .streamlit/config.toml to customize theme.
```

```
ğŸ“¤ Export & Evaluation Features:

    âœ… Faithfulness and relevance scores shown in collapsible sections.

    âœ… Evaluation results (JSON/CSV) downloadable via UI.

    âœ… Easily switch between auto-eval and manual evaluation.
```
```
âœï¸ Blogs & Write-ups
ğŸ“˜ Coming Soon on Hashnode

    â€œBuilding a Modular RAG Pipeline from Scratchâ€

    â€œHow to Evaluate RAG Systems Without OpenAIâ€

    â€œStreamlit + LangChain: Perfect UI for Document QAâ€
```
```
ğŸ¤ Contributing
    Contributions are welcome! Feel free to submit issues or pull requests.
```
```
ğŸ“œ License
    MIT License Â© Rahul Chauhan
```
```
ğŸ™Œ Acknowledgments
    LangChain for modular building blocks

    LangSmith for observability

    HuggingFace & Sentence-Transformers for embeddings

    Papers like ARES & RAGAS for evaluation inspiration
```
---

---

âœ… Next Steps (optional):
- I can save this as `README.md` and export it for your GitHub.
- Help you write the Render deployment guide mentioned above.
- Help you turn this into a blog post on Hashnode.


