import os
import streamlit as st
from rag_enginex import pipeline  # calling centralized pipeline logic

# Page config
st.set_page_config(page_title="ğŸ§  RAG-EngineX", layout="wide")

# Header
st.markdown(
    """
    <h1 style="text-align:center; color:#4A90E2;">ğŸ“„ RAG-EngineX: Modular PDF Q&A Chatbot</h1>
    <p style="text-align:center; font-size:18px;">
        Upload a PDF. Ask smart questions. Let <b>Gemini</b> answer using context retrieved by your own RAG pipeline.
    </p>
    """, unsafe_allow_html=True
)

st.markdown("---")

# Sidebar Settings
with st.sidebar:
    st.header("âš™ï¸ RAG Settings")
    chunk_size = st.slider("ğŸ”ª Chunk Size", 100, 2000, 800, step=100)
    chunk_overlap = st.slider("ğŸ” Chunk Overlap", 0, 500, 100, step=50)
    top_k = st.slider("ğŸ“š Top K Chunks", 1, 10, 5)
    rerank_top_n = st.slider("ğŸ¯ Top N After Rerank", 1, top_k, 3)
    st.markdown("---")
    st.caption("Made with â¤ï¸ for AI internships and beyond.")

# State setup
if "chunks" not in st.session_state:
    st.session_state.chunks = []
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "embedder" not in st.session_state:
    st.session_state.embedder = None

# Two-column layout
left, right = st.columns([1, 2])

with left:
    st.subheader("ğŸ“¤ Upload PDF")
    uploaded_pdf = st.file_uploader("Drop a PDF here", type=["pdf"])

    if uploaded_pdf:
        with st.spinner("ğŸ” Reading and processing..."):
            with open("temp.pdf", "wb") as f:
                f.write(uploaded_pdf.read())

            # ğŸ” Process PDF via pipeline
            chunks, embeddings, db, embed_model = pipeline.process_pdf(
                "temp.pdf",
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            st.session_state.chunks = chunks
            st.session_state.vector_store = db
            st.session_state.embedder = embed_model

        st.success(f"âœ… {len(chunks)} chunks embedded and indexed!")

        with st.expander("ğŸ“„ View Sample Chunks"):
            for i, chunk in enumerate(chunks[:5]):
                st.markdown(f"**Chunk {i+1}**: {chunk[:300]}...")

with right:
    st.subheader("ğŸ’¬ Ask a Question")
    question = st.text_input("Type your question about the PDF...", placeholder="E.g., What are the key projects mentioned?")

    if st.button("ğŸš€ Generate Answer", use_container_width=True):
        if not question:
            st.warning("Please enter a question first.")
        elif not st.session_state.vector_store:
            st.error("Upload and process a PDF before asking.")
        else:
            with st.spinner("ğŸ§  Thinking..."):
                # ğŸ” Full query pipeline
                answer, reranked_chunks = pipeline.process_query(
                    question,
                    st.session_state.vector_store,
                    st.session_state.embedder,
                    top_k=top_k,
                    rerank_top_n=rerank_top_n
                )

            st.success("âœ… Answer generated!")
            st.markdown("### ğŸ“¢ Answer")
            st.markdown(answer)

            with st.expander("ğŸ“š Reranked Context Chunks Used"):
                for i, chunk in enumerate(reranked_chunks):
                    st.markdown(f"**Chunk {i+1}:** {chunk[:400]}...")

st.markdown("---")
st.markdown("<center>âœ¨ Built with modularity, style, and ğŸ’¡ by RAG-EngineX âœ¨</center>", unsafe_allow_html=True)
